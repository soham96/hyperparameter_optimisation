{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimisation Using Gaussian Process\n",
    "\n",
    "In this repo, we use the `bayes_opt` package to perform Bayesian Optimisation Hyperparameter search. We will use this to try to find the optimal hyper-parameters for a FFNN based model and a CNN based model to classify mnist digits.\n",
    "\n",
    "We will use the following packages:\n",
    "- Keras\n",
    "- Numpy\n",
    "- bayes_opt\n",
    "\n",
    "### Import Packages\n",
    "\n",
    "First we will import the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GafrvarSjcwi",
    "outputId": "ddce7522-530c-429a-cd40-f8b77266234b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "we can use the `mnist.load_data()` to get the data.\n",
    "\n",
    "We then perform the following preprocessing:\n",
    "- We convert the images to float32 representation\n",
    "- We then normalise the images to range (0, 1) by dividing with 255\n",
    "- We one-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-PZz47xjf6O"
   },
   "outputs": [],
   "source": [
    "# Fetching the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Convert to Float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalise the data\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Convert labels to one-hot encoded representation\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "data=(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Models\n",
    "\n",
    "#### Feed Forward Neural Network\n",
    "First let's try to optimise a FF model\n",
    "\n",
    "This function defines the model. When using `bayes_opt`, we replace the different hyperparameters with variables that the `bayes_opt` then fills in for us during execution. Since there can be a lot of such hyperparameters, we pass them using `**kwargs`. Basically, it is a python dictionary with each key being an argument keyword and the value being the argument values.\n",
    "\n",
    "We keep the first layer parameters constant and change the consecutive parameters.\n",
    "The `for` loop is used to change the number of hidden layers. Inside the loop, we also change the number of neurons in each layer, whether dropout is used or not, and if used, the dropout percentage. Finally, we have an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5M8XvBZ0jhkv"
   },
   "outputs": [],
   "source": [
    "def make_ffnn_model(image_shape, **kwargs):\n",
    "    '''\n",
    "    Function to make the Feed-Forward Neural Network Model\n",
    "    Inputs:\n",
    "        image_shape: The shape of the input\n",
    "        **kwargs: The different parameters to optimise\n",
    "    Returns:\n",
    "        The model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(500, activation='relu', input_shape=image_shape, name='input'))\n",
    "    \n",
    "    #For Loop to change the number of layers\n",
    "    for _ in range(kwargs['layers']):\n",
    "        model.add(Dense(kwargs['neurons'], activation='relu')) #Changing the number of neurons\n",
    "        if kwargs['dropout']  == 1:                            #Whether dropout is applied or not\n",
    "            model.add(Dropout(kwargs['dropout_perc']))         #If dropout is applied then the percentage\n",
    "    model.add(Dense(10, activation='softmax', name='output'))  #Output layer\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network\n",
    "\n",
    "Similar to before we define a CNN. The different parameters optimised are:\n",
    "- The number of kernels in each Conv layer\n",
    "- The size of the kernel\n",
    "- Whether max pooling is applied. If yes, then we apply a (2,2) kernel with stride (2,2)\n",
    "- If Dropout is applied after each CNN layer.\n",
    "- If it is applied then the percentage\n",
    "- The number of Dense Layers\n",
    "- For each layer, the number of neurons\n",
    "- If Dropout is applied after each layer\n",
    "- If it is, then the percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLqGm_DZji7c"
   },
   "outputs": [],
   "source": [
    "def make_cnn_model(image_shape, **kwargs):\n",
    "    '''\n",
    "    Function to make the Convolutional Neural Network Model\n",
    "    Inputs:\n",
    "        image_shape: The shape of the input\n",
    "        **kwargs: The different parameters to optimise\n",
    "    Returns:\n",
    "        The model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(kwargs['kernels'], kernel_size=(kwargs['kernel_size'],kwargs['kernel_size']), activation='relu', input_shape=image_shape))\n",
    "    \n",
    "    for _ in range(kwargs['conv_layers']):\n",
    "        model.add(Conv2D(kwargs['kernels'], kernel_size=(kwargs['kernel_size'],kwargs['kernel_size']), activation='relu'))\n",
    "        if kwargs['maxpooling'] == 1:\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        if kwargs['dropout_cnn']  == 1:\n",
    "            model.add(Dropout(kwargs['dropout_perc_cnn']))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for _ in range(kwargs['layers']):\n",
    "        model.add(Dense(kwargs['neurons'], activation='relu'))\n",
    "        if kwargs['dropout']  == 1:\n",
    "            model.add(Dropout(kwargs['dropout_perc']))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Model for Training\n",
    "\n",
    "Before we can begin training, we need to specify how to train the model and the different parameters to train the model. This function is used to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_22Z5s_jkVi"
   },
   "outputs": [],
   "source": [
    "def fit_params(image_shape, data, **kwargs):\n",
    "    '''\n",
    "    Function to prepare the model for training\n",
    "    Input:\n",
    "        image_shape: The shape of the input to the model\n",
    "        data: A tuple of the form (Training_Features, Training_Labels, Testing_Features, Testing_Labels)\n",
    "        kwargs: The different parameters to optimise\n",
    "    Returns:\n",
    "        The Accuracy of the trained model given the parameters\n",
    "    '''\n",
    "    # Since most of our parameters are Integers (no. of layers, kernel size etc.)\n",
    "    # We convert everything to int, except the dropout percentage\n",
    "    for k in kwargs.keys():\n",
    "        if 'perc' in k:\n",
    "            continue\n",
    "        kwargs[k]=kwargs[k].astype(np.int64)\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = data\n",
    "    \n",
    "    # Reshape the input image to feed to the mode\n",
    "    # If the image_shape is 784, then we are training a FFNN and reshape the data as such.\n",
    "    # Else we reshape it to shape (28, 28, 1) (channel last format) as required by the \n",
    "    # TensorFlow backend\n",
    "    if image_shape[0]==784:\n",
    "        x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "        model = make_ffnn_model(image_shape, **kwargs)\n",
    "    else:   \n",
    "        x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "        model = make_cnn_model(image_shape, **kwargs)\n",
    "        \n",
    "    # We use categorical crossentropy as our loss and Adam as our optimiser\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Creating the path to save the trained models\n",
    "    if not os.path.exists(f'models/{image_shape[0]}'):\n",
    "        os.makedirs(f'models/{image_shape[0]}')\n",
    "    \n",
    "    # We use EarlyStopping to stop the training of the model if the validation accuracy does not increase\n",
    "    # after two epochs. ModelCheckpoint saves the best model that we trained.\n",
    "    # We monitor the validation loss in these callbacks.\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath=f'models/{image_shape[0]}/best.hdf5', monitor='val_loss', save_best_only=True)]\n",
    "    \n",
    "    # Train the model for a large number of epochs, so that the model stops due to the callbacks\n",
    "    # and not due to less epochs. We use a validation split of 20% of training data\n",
    "    model.fit(x_train, y_train, batch_size=kwargs['batch_size'], epochs=100, verbose=0, callbacks=callbacks, validation_split=0.2)\n",
    "    \n",
    "    # Evaluate the model with the test dataset.\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    # Save the best model\n",
    "    model.save(f'models/{image_shape[0]}/{score[1]}.hdf5')\n",
    "\n",
    "    # Return the accuracy.\n",
    "    return score[1]\n",
    "\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Set the image for training the FFNN model\n",
    "image_shape=(784, )\n",
    "\n",
    "fit_with_partial = partial(fit_params, image_shape, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Models\n",
    "\n",
    "#### FFNN\n",
    "First we train the FFNN model.\n",
    "\n",
    "We set the following parameter space:\n",
    "- Number of layers between 1 and 10\n",
    "- Number of neurons per layer between 50 and 500\n",
    "- Batch size between 16 and 64\n",
    "- dropout as either True or False\n",
    "- Dropout percentage between 0.1 and 0.5\n",
    "\n",
    "We can then use the `fit_with_partial` function to train our model using the parameter space we have set.\n",
    "\n",
    "We then run the optimiser. We run it for 4 iterations and initialise the parameters 3 times. Ofcourse running it for more will help it search more space and might help improve accuracy\n",
    "\n",
    "We print the hyperparameters for each iteration and also the best iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "colab_type": "code",
    "id": "vWI6spZhjl7R",
    "outputId": "9bd97b7f-4a41-42f3-be44-3a9d4f4227be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... |  dropout  | dropou... |  layers   |  neurons  |\n",
      "-------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.965   \u001b[0m | \u001b[0m 36.02   \u001b[0m | \u001b[0m 0.7203  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 3.721   \u001b[0m | \u001b[0m 116.0   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.9735  \u001b[0m | \u001b[95m 20.43   \u001b[0m | \u001b[95m 0.1863  \u001b[0m | \u001b[95m 0.2382  \u001b[0m | \u001b[95m 4.571   \u001b[0m | \u001b[95m 292.5   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9779  \u001b[0m | \u001b[95m 36.12   \u001b[0m | \u001b[95m 0.6852  \u001b[0m | \u001b[95m 0.1818  \u001b[0m | \u001b[95m 8.903   \u001b[0m | \u001b[95m 62.32   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.9749  \u001b[0m | \u001b[0m 64.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 500.0   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9684  \u001b[0m | \u001b[0m 16.09   \u001b[0m | \u001b[0m 0.5771  \u001b[0m | \u001b[0m 0.4721  \u001b[0m | \u001b[0m 2.037   \u001b[0m | \u001b[0m 498.3   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9746  \u001b[0m | \u001b[0m 63.41   \u001b[0m | \u001b[0m 0.5983  \u001b[0m | \u001b[0m 0.4071  \u001b[0m | \u001b[0m 9.913   \u001b[0m | \u001b[0m 51.26   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.972   \u001b[0m | \u001b[0m 16.39   \u001b[0m | \u001b[0m 0.04473 \u001b[0m | \u001b[0m 0.3786  \u001b[0m | \u001b[0m 9.952   \u001b[0m | \u001b[0m 408.0   \u001b[0m |\n",
      "=====================================================================================\n",
      "Iteration 0: \n",
      "\t{'target': 0.965, 'params': {'batch_size': 36.01705622572355, 'dropout': 0.7203244934421581, 'dropout_perc': 0.10004574992693796, 'layers': 3.7209931536865577, 'neurons': 116.04015086770087}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.9735, 'params': {'batch_size': 20.432252548902294, 'dropout': 0.1862602113776709, 'dropout_perc': 0.2382242908172191, 'layers': 4.570907268076029, 'neurons': 292.46753030151064}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.9779, 'params': {'batch_size': 36.12133669135815, 'dropout': 0.6852195003967595, 'dropout_perc': 0.18178089989260698, 'layers': 8.90305692751851, 'neurons': 62.32441693906677}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.9749, 'params': {'batch_size': 63.99999999986532, 'dropout': 0.9999999999921437, 'dropout_perc': 0.10000000000908028, 'layers': 9.999999999998312, 'neurons': 499.9999999999804}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.9684, 'params': {'batch_size': 16.08619296942139, 'dropout': 0.5770649810654986, 'dropout_perc': 0.4721178752279528, 'layers': 2.036632324277735, 'neurons': 498.3257779395662}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.9746, 'params': {'batch_size': 63.409661534433184, 'dropout': 0.5983233657937921, 'dropout_perc': 0.40710567062728364, 'layers': 9.913481025962007, 'neurons': 51.26003583908631}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.972, 'params': {'batch_size': 16.387760328375794, 'dropout': 0.044732696472810085, 'dropout_perc': 0.3786382104386893, 'layers': 9.952265619492389, 'neurons': 408.0064416052994}}\n",
      "{'target': 0.9779, 'params': {'batch_size': 36.12133669135815, 'dropout': 0.6852195003967595, 'dropout_perc': 0.18178089989260698, 'layers': 8.90305692751851, 'neurons': 62.32441693906677}}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'layers':(1,10), 'neurons':(50, 500), 'batch_size':(16, 64), 'dropout': (0,1), 'dropout_perc': (0.1,0.5)}\n",
    "\n",
    "# Setting up the optimiser\n",
    "optimizer = BayesianOptimization(\n",
    "    f=fit_with_partial,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "# Start the parameter search\n",
    "optimizer.maximize(init_points=3, n_iter=4)\n",
    "\n",
    "# Print the results for each iteration\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "# Print the result of the best model\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "As you can see from above, the best model is when we use:\n",
    "- batch size of 36\n",
    "- no dropout\n",
    "- 8 Dense Layers\n",
    "- 62 Neurons in each layer\n",
    "\n",
    "We acheive a testing accuracy of 97.79%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model\n",
    "Next we try to train the CNN model\n",
    "\n",
    "We set the following hyperparameters:\n",
    "- Convolutional layers between 1 and 5\n",
    "- Conv kernel numbers between 16 and 64\n",
    "- Conv kernel shape between 3 and 9\n",
    "- Whether to use Max Pooling or not\n",
    "- Whether to use dropout or not and if we then percentage between 0.1 and 0.5\n",
    "- Number of FC layers between 1 and 10\n",
    "- Number of neurons in each FC layer\n",
    "- If using dropout then percentage between 0.1 and 0.5\n",
    "- Batch size between 16 and 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "itkz211Ijsb7",
    "outputId": "94540160-63e4-4346-9ac8-9b8f6c2cb85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... | conv_l... |  dropout  | dropou... | dropou... | dropou... | kernel... |  kernels  |  layers   | maxpoo... |  neurons  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9883  \u001b[0m | \u001b[0m 36.02   \u001b[0m | \u001b[0m 3.881   \u001b[0m | \u001b[0m 0.000114\u001b[0m | \u001b[0m 0.3023  \u001b[0m | \u001b[0m 0.1587  \u001b[0m | \u001b[0m 0.1369  \u001b[0m | \u001b[0m 4.118   \u001b[0m | \u001b[0m 32.59   \u001b[0m | \u001b[0m 4.571   \u001b[0m | \u001b[0m 0.5388  \u001b[0m | \u001b[0m 238.6   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.9883  \u001b[0m | \u001b[0m 48.89   \u001b[0m | \u001b[0m 1.818   \u001b[0m | \u001b[0m 0.8781  \u001b[0m | \u001b[0m 0.02739 \u001b[0m | \u001b[0m 0.3682  \u001b[0m | \u001b[0m 0.2669  \u001b[0m | \u001b[0m 6.352   \u001b[0m | \u001b[0m 22.74   \u001b[0m | \u001b[0m 2.783   \u001b[0m | \u001b[0m 0.8007  \u001b[0m | \u001b[0m 485.7   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9894  \u001b[0m | \u001b[95m 31.04   \u001b[0m | \u001b[95m 3.769   \u001b[0m | \u001b[95m 0.8764  \u001b[0m | \u001b[95m 0.8946  \u001b[0m | \u001b[95m 0.134   \u001b[0m | \u001b[95m 0.1156  \u001b[0m | \u001b[95m 4.019   \u001b[0m | \u001b[95m 58.15   \u001b[0m | \u001b[95m 1.885   \u001b[0m | \u001b[95m 0.4211  \u001b[0m | \u001b[95m 481.1   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.9902  \u001b[0m | \u001b[95m 63.93   \u001b[0m | \u001b[95m 1.706   \u001b[0m | \u001b[95m 0.4525  \u001b[0m | \u001b[95m 0.344   \u001b[0m | \u001b[95m 0.4424  \u001b[0m | \u001b[95m 0.3072  \u001b[0m | \u001b[95m 8.916   \u001b[0m | \u001b[95m 61.03   \u001b[0m | \u001b[95m 4.646   \u001b[0m | \u001b[95m 0.5995  \u001b[0m | \u001b[95m 50.4    \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9879  \u001b[0m | \u001b[0m 16.72   \u001b[0m | \u001b[0m 4.225   \u001b[0m | \u001b[0m 0.06827 \u001b[0m | \u001b[0m 0.407   \u001b[0m | \u001b[0m 0.2125  \u001b[0m | \u001b[0m 0.3565  \u001b[0m | \u001b[0m 3.312   \u001b[0m | \u001b[0m 63.41   \u001b[0m | \u001b[0m 5.253   \u001b[0m | \u001b[0m 0.05927 \u001b[0m | \u001b[0m 55.21   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9867  \u001b[0m | \u001b[0m 63.9    \u001b[0m | \u001b[0m 4.467   \u001b[0m | \u001b[0m 0.3244  \u001b[0m | \u001b[0m 0.3529  \u001b[0m | \u001b[0m 0.3402  \u001b[0m | \u001b[0m 0.2444  \u001b[0m | \u001b[0m 3.591   \u001b[0m | \u001b[0m 63.97   \u001b[0m | \u001b[0m 5.44    \u001b[0m | \u001b[0m 0.6216  \u001b[0m | \u001b[0m 496.2   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.988   \u001b[0m | \u001b[0m 63.36   \u001b[0m | \u001b[0m 1.52    \u001b[0m | \u001b[0m 0.1986  \u001b[0m | \u001b[0m 0.6842  \u001b[0m | \u001b[0m 0.1275  \u001b[0m | \u001b[0m 0.4587  \u001b[0m | \u001b[0m 6.111   \u001b[0m | \u001b[0m 16.55   \u001b[0m | \u001b[0m 8.87    \u001b[0m | \u001b[0m 0.9884  \u001b[0m | \u001b[0m 58.99   \u001b[0m |\n",
      "=============================================================================================================================================================\n",
      "Iteration 0: \n",
      "\t{'target': 0.9883, 'params': {'batch_size': 36.01705622572355, 'conv_layers': 3.8812979737686324, 'dropout': 0.00011437481734488664, 'dropout_cnn': 0.30233257263183977, 'dropout_perc': 0.15870235632684523, 'dropout_perc_cnn': 0.13693543790751914, 'kernel_size': 4.117561268266026, 'kernels': 32.58691489806629, 'layers': 4.570907268076029, 'maxpooling': 0.538816734003357, 'neurons': 238.63753148148265}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.9883, 'params': {'batch_size': 48.890536019044454, 'conv_layers': 1.8178089989260697, 'dropout': 0.8781174363909454, 'dropout_cnn': 0.027387593197926163, 'dropout_perc': 0.36818700407136096, 'dropout_perc_cnn': 0.2669219209468508, 'kernel_size': 6.35213897067451, 'kernels': 22.73857305257122, 'layers': 2.782913401763909, 'maxpooling': 0.8007445686755367, 'neurons': 485.7177090737289}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.9894, 'params': {'batch_size': 31.044360551643656, 'conv_layers': 3.7692904626772563, 'dropout': 0.8763891522960383, 'dropout_cnn': 0.8946066635038473, 'dropout_perc': 0.1340176845479112, 'dropout_perc_cnn': 0.11562191329315295, 'kernel_size': 4.018982517387413, 'kernels': 58.15084016461183, 'layers': 1.8851215044974508, 'maxpooling': 0.42110762500505217, 'neurons': 481.0502885677259}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.9902, 'params': {'batch_size': 63.93068741637696, 'conv_layers': 1.7060355964461502, 'dropout': 0.4524658705948822, 'dropout_cnn': 0.34398454308885473, 'dropout_perc': 0.442352372139471, 'dropout_perc_cnn': 0.30724922331982235, 'kernel_size': 8.915753411817628, 'kernels': 61.02575960461968, 'layers': 4.6464443247901785, 'maxpooling': 0.5994933136855431, 'neurons': 50.401458263610635}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.9879, 'params': {'batch_size': 16.724223351357207, 'conv_layers': 4.225272897790595, 'dropout': 0.06826985800197649, 'dropout_cnn': 0.40696158825076445, 'dropout_perc': 0.2125406357761635, 'dropout_perc_cnn': 0.3564795998132638, 'kernel_size': 3.312036431572106, 'kernels': 63.40735073747851, 'layers': 5.25272623613516, 'maxpooling': 0.059265040898343835, 'neurons': 55.20773343425527}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.9867, 'params': {'batch_size': 63.897992325650115, 'conv_layers': 4.467068257622415, 'dropout': 0.3243563906362662, 'dropout_cnn': 0.3528825780425515, 'dropout_perc': 0.3401608702819013, 'dropout_perc_cnn': 0.24435220032906402, 'kernel_size': 3.5906409808648925, 'kernels': 63.96645967747607, 'layers': 5.44010623636822, 'maxpooling': 0.6216428889673895, 'neurons': 496.1933497710863}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.988, 'params': {'batch_size': 63.364452616465684, 'conv_layers': 1.5196878590693013, 'dropout': 0.19856095619989333, 'dropout_cnn': 0.6842470603817751, 'dropout_perc': 0.12751824191041702, 'dropout_perc_cnn': 0.45867732813648354, 'kernel_size': 6.111190455995128, 'kernels': 16.552614070665253, 'layers': 8.87045751698265, 'maxpooling': 0.9883921498485446, 'neurons': 58.98607325200158}}\n",
      "{'target': 0.9902, 'params': {'batch_size': 63.93068741637696, 'conv_layers': 1.7060355964461502, 'dropout': 0.4524658705948822, 'dropout_cnn': 0.34398454308885473, 'dropout_perc': 0.442352372139471, 'dropout_perc_cnn': 0.30724922331982235, 'kernel_size': 8.915753411817628, 'kernels': 61.02575960461968, 'layers': 4.6464443247901785, 'maxpooling': 0.5994933136855431, 'neurons': 50.401458263610635}}\n"
     ]
    }
   ],
   "source": [
    "image_shape=(28, 28, 1)\n",
    "\n",
    "\n",
    "fit_with_partial = partial(fit_params, image_shape, data)\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'layers':(1,10), 'neurons':(50, 500), 'batch_size':(16, 64), 'dropout': (0,1), 'dropout_perc': (0.1,0.5), \n",
    "          'dropout_perc_cnn':(0.1, 0.5), 'dropout_cnn':(0,1), 'maxpooling': (0,1), 'conv_layers':(1,5), \n",
    "           'kernel_size':(3, 9), 'kernels':(16, 64)}\n",
    "\n",
    "# Setting up the optimiser\n",
    "optimizer = BayesianOptimization(\n",
    "    f=fit_with_partial,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "# Start the parameter search\n",
    "optimizer.maximize(init_points=3, n_iter=4)\n",
    "\n",
    "# Print the results for each iteration\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "# Print the result of the best model\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "The best model gets an accuracy of 99.02% which is much higher than the FFNN. You can see the final hyperparameters above\n",
    "\n",
    "## Conclusion\n",
    "This notebook shows how to use Bayesian Optimisation and the `bayes_opt` package to do hyperparameter search. Better results can be obtained by changing the hyperparameter bounds and increase the number of iterations we run it for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bayesian_Optimisation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
