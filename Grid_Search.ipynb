{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimisation using Grid Search\n",
    "\n",
    "In this repo, we use `hyperas` package to perform Grid Search Hyperparameter search. We will use this to try to find the optimal hyper-parameters for a FFNN based model and a CNN based model to classify mnist digits.\n",
    "\n",
    "We will use the following packages:\n",
    "- Keras\n",
    "- Numpy\n",
    "- hyperas\n",
    "\n",
    "### Import Packages\n",
    "\n",
    "First we will install and import the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 940
    },
    "colab_type": "code",
    "id": "ICvZhdCIuli_",
    "outputId": "f828fb82-cf30-458e-cebd-ac4d72b54ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperas in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.6.0)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
      "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
      "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.5)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10.1)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.1.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.5.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.2)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.12.0)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.9.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.16.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.3.1)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.0)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.5.5)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.5.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.8)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->hyperas) (4.4.0)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (5.5.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (1.0.16)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (5.3.3)\n",
      "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->hyperas) (4.5.3)\n",
      "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.8.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (41.2.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (4.7.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.7.5)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter->hyperas) (2.5.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter->hyperas) (17.0.0)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperas\n",
    "\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is some boilerplate code to help hyperas run on google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJUOUnP098tL"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "fid = drive.ListFile({'q':\"title='Grid_Search.ipynb'\"}).GetList()[0]['id']\n",
    "f = drive.CreateFile({'id': fid})\n",
    "f.GetContentFile('Grid_Search.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "#### For FFNN\n",
    "Here we define a function that will grab our data, preprocess it and then send it to our model.\n",
    "\n",
    "We perform the following preprocessing steps:\n",
    "- We convert the image to float32\n",
    "- We normalise it to the range (0, 1) by dividing it with 255\n",
    "- We reshape it to (784) to feed it into a Dense Layer\n",
    "- We one-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dk9Q6URkwnQS"
   },
   "outputs": [],
   "source": [
    "def data_ffnn():\n",
    "    '''\n",
    "    Function to preprocess mnist data so that it can be fed into a FFNN\n",
    "    Returns:\n",
    "        x_train: Training Images\n",
    "        y_train: Training Labels\n",
    "        x_test: Testing Images\n",
    "        y_test: Testing Labels\n",
    "    '''\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()    #Loading the data\n",
    "\n",
    "    #Converting it to Float32\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    #Normalising it by dividing with 255\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    # One-Hot Encode the Labels\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    #Reshaping the Images\n",
    "    x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For CNN\n",
    "Here we perform similar preprocessing to train the CNN.\n",
    "The only difference is that we reshape the data to shape (28, 28, 1) which is the channels last format that TensorFlow uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0P6lZS2g4KZ"
   },
   "outputs": [],
   "source": [
    "def data_cnn():\n",
    "    '''\n",
    "    Function to preprocess mnist data so that it can be fed into a CNN\n",
    "    Returns:\n",
    "        x_train: Training Images\n",
    "        y_train: Training Labels\n",
    "        x_test: Testing Images\n",
    "        y_test: Testing Labels\n",
    "    '''\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()  #Loading the data\n",
    "    \n",
    "    #Converting it to Float32\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    #Normalising it by dividing it with 255\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    #One-Hot Encode the labels\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    #Reshaping the data to feed it to the CNN\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Models\n",
    "\n",
    "#### Feed Forward Neural Network\n",
    "In Hyperas, the backend searches for the `{{}}` tag and replaces what is inside with values in different runs\n",
    "When using `choice`, it chooses between the given set of hyperparameters\n",
    "When using `uniform`, it uses values at steps given in the range\n",
    "\n",
    "We keep the first layer parameters as a choice between 256, 512 and 1024 neurons. The `for` loop is used to change the number of hidden layers (choice of 2, 4 and 6). Inside the loop, we also change the number of neurons in each layer, whether dropout is used or not, and if used, the dropout percentage. Finally, we have an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQ4OfiKY6osg"
   },
   "outputs": [],
   "source": [
    "def create_ffnn_model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Function to create and compile the FFNN\n",
    "    Args:\n",
    "        x_train: Training Images\n",
    "        y_train: Training Labels\n",
    "        x_test: Testing Images\n",
    "        y_test: Testing Labels\n",
    "    Returns:\n",
    "        Dictionary with loss, status and trained model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Using choice to choose one of 256, 512 and 1024 neurons for each run.\n",
    "    model.add(Dense({{choice([256, 512, 1024])}}, activation='relu', input_shape=(784,), name='input'))\n",
    "    #Choosing the number of layers in the output.\n",
    "    for _ in range({{choice([2, 4, 6])}}):\n",
    "        model.add(Dense({{choice([32, 64, 128])}}, activation='relu'))\n",
    "        #Choosing to apply or not apply dropout\n",
    "        if {{choice([0, 1])}}  == 1:\n",
    "            model.add(Dropout({{uniform(0.1, 0.5)}}))\n",
    "    model.add(Dense(10, activation='softmax', name='output'))\n",
    "    \n",
    "    # We use categorical crossentropy as our loss and Adam as our optimiser\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    #Creating the path to save the models to\n",
    "    if not os.path.exists(f'models/ffnn'):\n",
    "        os.makedirs(f'models/ffnn')\n",
    "\n",
    "    # We use EarlyStopping to stop the training of the model if the validation accuracy does not increase\n",
    "    # after two epochs. ModelCheckpoint saves the best model that we trained.\n",
    "    # We monitor the validation loss in these callbacks.\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath=f'models/ffnn/best.hdf5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "    # Train the model for a large number of epochs, so that the model stops due to the callbacks\n",
    "    # and not due to less epochs. We use a validation split of 20% of training data. We also give\n",
    "    # a choice of 16, 32 or 64 bathsize\n",
    "    model.fit(x_train, y_train, batch_size={{choice([16, 32, 64])}}, epochs=100, verbose=0, callbacks=callbacks, validation_split=0.2)\n",
    "    \n",
    "    # Evaluate the model with the eval dataset.\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    #Save the best model\n",
    "    model.save(f'models/ffnn/{score[1]}.hdf5')\n",
    "    \n",
    "    #Boilerplate Hyperas code to give it info on the model performance\n",
    "    return {'loss': -score[1], 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model\n",
    "\n",
    "We give the model the following hyperparameter search space:\n",
    "- Convolutional layers 2 or 4\n",
    "- Conv kernel numbers 8, 12, 16 in the first layer and 2, 4 in the subsequent layers\n",
    "- Conv kernel shape of 3, 5, 7 in the first layer and 3, 5 in the second layer\n",
    "- Whether to use Max Pooling or not\n",
    "- Whether to use dropout or not and if we then percentage between 0.1 and 0.5\n",
    "- Number of FC layers of choice 2, 4, 6\n",
    "- Number of neurons in each FC layer of choice 512, 1024 and 128\n",
    "- If using dropout then percentage between 0.1 and 0.5\n",
    "- Batch size of either 16, 32 and 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SBkBBbxKyAvx"
   },
   "outputs": [],
   "source": [
    "def create_cnn_model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Function to create and compile the CNN\n",
    "    Args:\n",
    "        x_train: Training Images\n",
    "        y_train: Training Labels\n",
    "        x_test: Testing Images\n",
    "        y_test: Testing Labels\n",
    "    Returns:\n",
    "        Dictionary with loss, status and trained model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Choice of 8, 12, 16 for conv kernels and 3, 5, 7 for kernel size\n",
    "    model.add(Conv2D({{choice([8, 12, 16])}}, kernel_size={{choice([(3,3), (5,5), (7,7)])}}, activation='relu', input_shape=(28, 28, 1), name='input'))\n",
    "\n",
    "    #Choice of number of conv layers and whether to apply or not apply pooling and dropout\n",
    "    for _ in range({{choice([2, 4])}}):\n",
    "        model.add(Conv2D({{choice([2, 4])}}, kernel_size={{choice([(3,3), (5,5)])}}, activation='relu'))\n",
    "        if {{choice([0, 1])}}  == 1:\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        if {{choice([0, 1])}}  == 1:\n",
    "            model.add(Dropout({{uniform(0.1, 0.5)}}))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Choice of number of Dense layers and whether to use Dropout or not\n",
    "    for _ in range({{choice([2, 4, 6])}}):\n",
    "        model.add(Dense({{choice([512, 1024, 128])}}, activation='relu'))\n",
    "        if {{choice([0, 1])}}  == 1:\n",
    "            model.add(Dropout({{uniform(0.1, 0.5)}}))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax', name='output'))\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    if not os.path.exists(f'models/cnn'):\n",
    "        os.makedirs(f'models/cnn')\n",
    "    \n",
    "    # We use EarlyStopping to stop the training of the model if the validation accuracy does not increase\n",
    "    # after two epochs. ModelCheckpoint saves the best model that we trained.\n",
    "    # We monitor the validation loss in these callbacks.\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath=f'models/cnn/best.hdf5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size={{choice([16, 32, 64])}}, epochs=100, verbose=0, callbacks=callbacks, validation_split=0.2)\n",
    "    \n",
    "    # Evaluate the model with the eval dataset.\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    model.save(f'models/ffnn/{score[1]}.hdf5')\n",
    "    \n",
    "    return {'loss': -score[1], 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Training and Parameter Search\n",
    "\n",
    "#### FFNN Training\n",
    "The `optim.minimise` function tries to find the model with the least loss.\n",
    "We feed it the model function, the data loading function and the number of evals to run for. Running for more evals will take more time, but will give better results.\n",
    "\n",
    "The we evaluate the best model on the test dataset and see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4C_NqyhSx_PF",
    "outputId": "c796b8ce-efcb-4e49-c9d8-bed57cf90ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from pydrive.auth import GoogleAuth\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from pydrive.drive import GoogleDrive\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from google.colab import auth\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from oauth2client.client import GoogleCredentials\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [256, 512, 1024]),\n",
      "        'range': hp.choice('range', [2, 4, 6]),\n",
      "        'Dense_1': hp.choice('Dense_1', [32, 64, 128]),\n",
      "        'Dense_2': hp.choice('Dense_2', [0, 1]),\n",
      "        'Dropout': hp.uniform('Dropout', 0.1, 0.5),\n",
      "        'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
      "  3: \n",
      "  4: x_train = x_train.astype('float32')\n",
      "  5: x_test = x_test.astype('float32')\n",
      "  6: x_train /= 255\n",
      "  7: x_test /= 255\n",
      "  8: \n",
      "  9: y_train = keras.utils.to_categorical(y_train, 10)\n",
      " 10: y_test = keras.utils.to_categorical(y_test, 10)\n",
      " 11: x_train = x_train.reshape(x_train.shape[0], 784)\n",
      " 12: x_test = x_test.reshape(x_test.shape[0], 784)\n",
      " 13: \n",
      " 14: \n",
      " 15: \n",
      " 16: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     model = Sequential()\n",
      "   4:     model.add(Dense(space['Dense'], activation='relu', input_shape=(784,), name='input'))\n",
      "   5:     for _ in range(space['range']):\n",
      "   6:         model.add(Dense(space['Dense_1'], activation='relu'))\n",
      "   7:         if space['Dense_2']  == 1:\n",
      "   8:             model.add(Dropout(space['Dropout']))\n",
      "   9:     model.add(Dense(10, activation='softmax', name='output'))\n",
      "  10:     \n",
      "  11:     model.compile(loss=keras.losses.categorical_crossentropy,\n",
      "  12:               optimizer=keras.optimizers.Adam(),\n",
      "  13:               metrics=['accuracy'])\n",
      "  14:     \n",
      "  15:     if not os.path.exists(f'models/ffnn'):\n",
      "  16:         os.makedirs(f'models/ffnn')\n",
      "  17:     \n",
      "  18:     callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
      "  19:              ModelCheckpoint(filepath=f'models/ffnn/best.hdf5', monitor='val_loss', save_best_only=True)]\n",
      "  20: \n",
      "  21:     model.fit(x_train, y_train, batch_size=space['batch_size'], epochs=100, verbose=0, callbacks=callbacks, validation_split=0.2)\n",
      "  22:     \n",
      "  23:     # Evaluate the model with the eval dataset.\n",
      "  24:     score = model.evaluate(x_test, y_test, verbose=0)\n",
      "  25:     \n",
      "  26:     model.save(f'models/ffnn/{score[1]}.hdf5')\n",
      "  27:     \n",
      "  28:     return {'loss': -score[1], 'status': STATUS_OK, 'model': model}\n",
      "  29: \n",
      "100%|██████████| 5/5 [05:28<00:00, 68.28s/it, best loss: -0.9799]\n",
      "10000/10000 [==============================] - 1s 74us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09332765673322065, 0.9799]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run the hyperparameter Search\n",
    "best_run, best_model = optim.minimize(model=create_ffnn_model,\n",
    "                                          data=data_ffnn,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                     notebook_name='Grid_Search')\n",
    "\n",
    "#Test the best model\n",
    "best_model.evaluate(data_ffnn)[2], data_ffnn()[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, the best result is about 98% accuracy\n",
    "\n",
    "#### CNN Training and Search\n",
    "Similar to the FFNN training, we do it for the CNN. In this case, we feed in different data and model functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VwgsK7NBiNPR",
    "outputId": "debf1453-0851-472f-f477-8adbe82f7455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from pydrive.auth import GoogleAuth\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from pydrive.drive import GoogleDrive\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from google.colab import auth\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from oauth2client.client import GoogleCredentials\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Conv2D': hp.choice('Conv2D', [8, 12, 16]),\n",
      "        'kernel_size': hp.choice('kernel_size', [(3,3), (5,5), (7,7)]),\n",
      "        'range': hp.choice('range', [2, 4]),\n",
      "        'range_1': hp.choice('range_1', [2, 4]),\n",
      "        'kernel_size_1': hp.choice('kernel_size_1', [(3,3), (5,5)]),\n",
      "        'kernel_size_2': hp.choice('kernel_size_2', [0, 1]),\n",
      "        'kernel_size_3': hp.choice('kernel_size_3', [0, 1]),\n",
      "        'Dropout': hp.uniform('Dropout', 0.1, 0.5),\n",
      "        'range_2': hp.choice('range_2', [2, 4, 6]),\n",
      "        'Dense': hp.choice('Dense', [512, 1024, 128]),\n",
      "        'Dense_1': hp.choice('Dense_1', [0, 1]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0.1, 0.5),\n",
      "        'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
      "  3: \n",
      "  4: x_train = x_train.astype('float32')\n",
      "  5: x_test = x_test.astype('float32')\n",
      "  6: x_train /= 255\n",
      "  7: x_test /= 255\n",
      "  8: \n",
      "  9: y_train = keras.utils.to_categorical(y_train, 10)\n",
      " 10: y_test = keras.utils.to_categorical(y_test, 10)\n",
      " 11: x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
      " 12: x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
      " 13: \n",
      " 14: \n",
      " 15: \n",
      " 16: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     model = Sequential()\n",
      "   4:     model.add(Conv2D(space['Conv2D'], kernel_size=space['kernel_size'], activation='relu', input_shape=(28, 28, 1), name='input'))\n",
      "   5:     for _ in range(space['range']):\n",
      "   6:         model.add(Conv2D(space['range_1'], kernel_size=space['kernel_size_1'], activation='relu'))\n",
      "   7:         if space['kernel_size_2']  == 1:\n",
      "   8:             model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "   9:         if space['kernel_size_3']  == 1:\n",
      "  10:             model.add(Dropout(space['Dropout']))\n",
      "  11:     model.add(Flatten())\n",
      "  12:     \n",
      "  13:     for _ in range(space['range_2']):\n",
      "  14:         model.add(Dense(space['Dense'], activation='relu'))\n",
      "  15:         if space['Dense_1']  == 1:\n",
      "  16:             model.add(Dropout(space['Dropout_1']))\n",
      "  17:     \n",
      "  18:     model.add(Dense(10, activation='softmax', name='output'))\n",
      "  19:     \n",
      "  20:     model.compile(loss=keras.losses.categorical_crossentropy,\n",
      "  21:               optimizer=keras.optimizers.Adam(),\n",
      "  22:               metrics=['accuracy'])\n",
      "  23:     \n",
      "  24:     if not os.path.exists(f'models/cnn'):\n",
      "  25:         os.makedirs(f'models/cnn')\n",
      "  26:     \n",
      "  27:     callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
      "  28:              ModelCheckpoint(filepath=f'models/cnn/best.hdf5', monitor='val_loss', save_best_only=True)]\n",
      "  29: \n",
      "  30:     model.fit(x_train, y_train, batch_size=space['batch_size'], epochs=100, verbose=0, callbacks=callbacks, validation_split=0.2)\n",
      "  31:     \n",
      "  32:     # Evaluate the model with the eval dataset.\n",
      "  33:     score = model.evaluate(x_test, y_test, verbose=0)\n",
      "  34:     \n",
      "  35:     model.save(f'models/ffnn/{score[1]}.hdf5')\n",
      "  36:     \n",
      "  37:     return {'loss': -score[1], 'status': STATUS_OK, 'model': model}\n",
      "  38: \n",
      "  0%|          | 0/5 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "100%|██████████| 5/5 [18:04<00:00, 203.19s/it, best loss: -0.978]\n",
      "10000/10000 [==============================] - 1s 144us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1096816274739273, 0.978]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_cnn_model,\n",
    "                                          data=data_cnn,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                     notebook_name='Grid_Search')\n",
    "\n",
    "best_model.evaluate(data_cnn()[2], data_cnn()[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model acheives and accuracy of 97.8%\n",
    "\n",
    "## Result\n",
    "This was a brute force attempt at hyperparameter search. Using Bayesian Optimisation, we can acheive better results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Grid_Search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
